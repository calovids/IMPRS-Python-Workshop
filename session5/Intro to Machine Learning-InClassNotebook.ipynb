{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ce916-aece-4ede-a4a3-2c51acf18179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041c574-a694-4d47-95a3-0657a89a2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from URL\n",
    "# url = \"https://zenodo.org/record/7298798/files/daydatamat.csv\"\n",
    "# df = pd.read_csv(url,index_col=0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b363756-0faa-4ed2-960a-e3429ed6cd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Day number</th>\n",
       "      <th>Bee unique ID</th>\n",
       "      <th>Cohort ID</th>\n",
       "      <th>Honey</th>\n",
       "      <th>Brood care</th>\n",
       "      <th>Pollen</th>\n",
       "      <th>Dance floor</th>\n",
       "      <th>Other</th>\n",
       "      <th>Frame 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Frame 2 - Num. crossings</th>\n",
       "      <th>Frame 3 - Num. crossings</th>\n",
       "      <th>Frame 4 - Num. crossings</th>\n",
       "      <th>Frame 5 - Num. crossings</th>\n",
       "      <th>Dispersion-minute avg.</th>\n",
       "      <th>Speed circadian coeff.</th>\n",
       "      <th>Time outside day5min</th>\n",
       "      <th>Num. outside trips day5min</th>\n",
       "      <th>Time outside day1min</th>\n",
       "      <th>Num. outside trips day1min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045395</td>\n",
       "      <td>0.212332</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733829</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>116.373039</td>\n",
       "      <td>-0.509029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036709</td>\n",
       "      <td>0.187211</td>\n",
       "      <td>0.048712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727368</td>\n",
       "      <td>0.021694</td>\n",
       "      <td>...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>156.944835</td>\n",
       "      <td>-0.565287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>0.357586</td>\n",
       "      <td>0.042301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>152.676205</td>\n",
       "      <td>-0.624841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.199898</td>\n",
       "      <td>0.162891</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.601880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>167.909343</td>\n",
       "      <td>-0.580714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.068730</td>\n",
       "      <td>0.160821</td>\n",
       "      <td>0.045910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.724540</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>140.118809</td>\n",
       "      <td>-0.418582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202704</th>\n",
       "      <td>33.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8095.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202705</th>\n",
       "      <td>33.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202706</th>\n",
       "      <td>29.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8277.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202707</th>\n",
       "      <td>29.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8427.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202708</th>\n",
       "      <td>29.0</td>\n",
       "      <td>115</td>\n",
       "      <td>8436.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202709 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Age  Day number  Bee unique ID  Cohort ID     Honey  Brood care  \\\n",
       "0       22.0           0            4.0        1.0  0.045395    0.212332   \n",
       "1       22.0           0            6.0        1.0  0.036709    0.187211   \n",
       "2       22.0           0            7.0        1.0  0.255499    0.357586   \n",
       "3       22.0           0            9.0        1.0  0.199898    0.162891   \n",
       "4       22.0           0           11.0        1.0  0.068730    0.160821   \n",
       "...      ...         ...            ...        ...       ...         ...   \n",
       "202704  33.0         115         8095.0       29.0  0.000000    0.000000   \n",
       "202705  33.0         115         8144.0       29.0  0.000000    0.371179   \n",
       "202706  29.0         115         8277.0       30.0  0.000000    0.000000   \n",
       "202707  29.0         115         8427.0       30.0  0.972222    0.027778   \n",
       "202708  29.0         115         8436.0       30.0  0.000000    0.000000   \n",
       "\n",
       "          Pollen  Dance floor     Other   Frame 0  ...  \\\n",
       "0       0.008444          0.0  0.733829  0.000775  ...   \n",
       "1       0.048712          0.0  0.727368  0.021694  ...   \n",
       "2       0.042301          0.0  0.344614  0.000000  ...   \n",
       "3       0.035332          0.0  0.601880  0.000000  ...   \n",
       "4       0.045910          0.0  0.724540  0.004808  ...   \n",
       "...          ...          ...       ...       ...  ...   \n",
       "202704  0.000000          0.0  1.000000  0.000000  ...   \n",
       "202705  0.000000          0.0  0.628821  0.000000  ...   \n",
       "202706  0.000000          0.0  1.000000  0.000000  ...   \n",
       "202707  0.000000          0.0  0.000000  0.000000  ...   \n",
       "202708  0.000000          0.0  1.000000  0.000000  ...   \n",
       "\n",
       "        Frame 2 - Num. crossings  Frame 3 - Num. crossings  \\\n",
       "0                           40.0                       0.0   \n",
       "1                           84.0                       2.0   \n",
       "2                           71.0                       0.0   \n",
       "3                           43.0                       1.0   \n",
       "4                           55.0                       4.0   \n",
       "...                          ...                       ...   \n",
       "202704                       0.0                       0.0   \n",
       "202705                       0.0                       0.0   \n",
       "202706                       0.0                       0.0   \n",
       "202707                       0.0                       0.0   \n",
       "202708                       0.0                       0.0   \n",
       "\n",
       "        Frame 4 - Num. crossings  Frame 5 - Num. crossings  \\\n",
       "0                           20.0                      46.0   \n",
       "1                           36.0                     104.0   \n",
       "2                           83.0                      32.0   \n",
       "3                           29.0                      63.0   \n",
       "4                           61.0                      45.0   \n",
       "...                          ...                       ...   \n",
       "202704                       0.0                       0.0   \n",
       "202705                       0.0                       0.0   \n",
       "202706                       0.0                       1.0   \n",
       "202707                       0.0                       0.0   \n",
       "202708                       1.0                       0.0   \n",
       "\n",
       "        Dispersion-minute avg.  Speed circadian coeff.  Time outside day5min  \\\n",
       "0                   116.373039               -0.509029                   0.0   \n",
       "1                   156.944835               -0.565287                   0.0   \n",
       "2                   152.676205               -0.624841                   0.0   \n",
       "3                   167.909343               -0.580714                   0.0   \n",
       "4                   140.118809               -0.418582                   0.0   \n",
       "...                        ...                     ...                   ...   \n",
       "202704                     NaN                0.382683                   0.0   \n",
       "202705                     NaN                0.382683                   0.0   \n",
       "202706                     NaN                0.382683                   0.0   \n",
       "202707                     NaN                0.382683                   0.0   \n",
       "202708                     NaN                0.382683                   0.0   \n",
       "\n",
       "        Num. outside trips day5min  Time outside day1min  \\\n",
       "0                              0.0              0.010417   \n",
       "1                              0.0              0.004167   \n",
       "2                              0.0              0.000000   \n",
       "3                              0.0              0.000000   \n",
       "4                              0.0              0.000000   \n",
       "...                            ...                   ...   \n",
       "202704                         0.0              0.000000   \n",
       "202705                         0.0              0.000000   \n",
       "202706                         0.0              0.000000   \n",
       "202707                         0.0              0.000000   \n",
       "202708                         0.0              0.000000   \n",
       "\n",
       "        Num. outside trips day1min  \n",
       "0                             11.0  \n",
       "1                              6.0  \n",
       "2                              0.0  \n",
       "3                              0.0  \n",
       "4                              0.0  \n",
       "...                            ...  \n",
       "202704                         0.0  \n",
       "202705                         0.0  \n",
       "202706                         0.0  \n",
       "202707                         0.0  \n",
       "202708                         0.0  \n",
       "\n",
       "[202709 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use local file \n",
    "path = '../data/daydatamat.csv'\n",
    "df = pd.read_csv(path,index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c81453-184f-4ab2-857e-25a602676317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2028, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a subset for use in model fitting in these examples\n",
    "dfsmall = df[::100]\n",
    "dfsmall.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd37b6-7217-47c7-b1b8-3b07d8b3bdb1",
   "metadata": {},
   "source": [
    "Types of Machine learning algorithms:\n",
    "1) Supervised\n",
    "2) Unsupervised\n",
    "3) Semi-supervised\n",
    "4) Reinforcement learning\n",
    "\n",
    "Parts of the ML process:\n",
    "- Representation\n",
    "- Optimization (fitting)\n",
    "-  Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211a3ea-f9de-41e6-8efe-5999d42be5c9",
   "metadata": {},
   "source": [
    "# Supervised learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa899a6-c544-4328-aaa0-72917553679f",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321770ce-7c43-431c-8ac9-ec2e39f87f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"Median speed\",y=\"Dispersion (avg)\",data=dfsmall,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d600015e-4891-4b65-9c5b-660315b9539f",
   "metadata": {},
   "source": [
    "Now lets do a fit using sklearn and \"machine-learning\"/\"model-fitting\" syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d72333-17b4-4355-afaa-8661b38b15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = sklearn.linear_model.LinearRegression()\n",
    "X = dfsmall[['Median speed']]\n",
    "y = dfsmall[['Dispersion (avg)']]\n",
    "#  use this to normalize the input/output values before fitting\n",
    "X = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "y = (y-np.mean(y,axis=0))/np.std(y,axis=0)\n",
    "reg.fit(X,y)\n",
    "print ('Coefficients: ', reg.coef_)\n",
    "print ('Intercept: ',reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e830517-b635-4838-968e-6cf47af3c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X, reg.coef_[0][0]*X + reg.intercept_[0], color='purple',linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40769c36-d223-4dd0-a047-914383302ceb",
   "metadata": {},
   "source": [
    "1. **Representation**.\n",
    "The equation for linear regression is \n",
    "$$y_i=b_0 + b_1x_i,$$\n",
    "where $x_i$ is input data, $y_i^*$ is the output prediction, and $b_0,b_1$ are fit coefficients.  The actual output is $y_i$.\n",
    "The mean square error is\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i))^2$$\n",
    "\n",
    "2. **Optimization**  (also called \"fitting\", or \"training\")\n",
    "Represent the input data as follows:  $X_i = (x_i^0,x_i^1)$  (so, then $\\mathbf{X}$ has dimensions of $(n,2)$.  Considering also the coefficient vector $\\beta=(b_0,b_1)$, the values of $\\beta$ that minimize the MSE can be explicitly solved for using linear algebra.  The solution is:\n",
    "$$\\beta^\\text{fit} = \\left(\\mathbf{X^T}\\cdot\\mathbf{X}\\right)^{-1}\\cdot\\mathbf{X^T}\\cdot \\mathbf{y}$$\n",
    "We can indeed calculate this and verify that it gives the same thing as using the sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5124b-397d-428c-a558-ac794c61dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtemp = np.reshape(np.array([X**0,X**1]),(2,len(X))).T\n",
    "beta_fit = np.dot(np.dot(np.linalg.inv(np.dot(Xtemp.T,Xtemp)),Xtemp.T),np.reshape(y.values,len(y)))\n",
    "print('beta_fit:',beta_fit)\n",
    "print('sklearn:',(reg.intercept_,reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5731c1b1-af4d-49d2-88f9-1164c25902b0",
   "metadata": {},
   "source": [
    "* In practice, its better to just use the built-in functions for fitting regression coefficients\n",
    "* This illlustrates a key concept though:  the process of how to fit the coefficients\n",
    "* For linear regression, the model is simple and has an analytical solution\n",
    "* For more complicated models, there is no analytical solution and more involved methods must be used for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b68c7a-0976-4989-85cd-13120a8f012f",
   "metadata": {},
   "source": [
    "3. **Evaluation** This is how to measure whether a not the model is a good one for the problem.  For regression-like models, common metrics used are:\n",
    "* MSE: this is the value of the cost function that was minimized to fit the model\n",
    "* $R^2$:  \"R-squared\", or coefficient of determination. It measures the proportion of the variance in the dependent variable that is predictable from the independent variables. Has a range of 0 to 1 (although negative values are theoretically possible with, for example, a purposefully poor model fit), with higher values meaning a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147ee23-1e86-4ed3-b47a-2958102c27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X)\n",
    "print('MSE:',sklearn.metrics.mean_squared_error(y,y_pred))\n",
    "print('r^2 score:',sklearn.metrics.r2_score(y,y_pred))\n",
    "# note:  r^2 can also be computed from:   reg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec776628-7168-4ad8-a5fd-de28ab1c1138",
   "metadata": {},
   "source": [
    "Without normalization, the MSE is not very meaningful.  In this example, if the input and output variables are normalized, then the MSE is equal to 1 minus r^2.  This relationship is specific to linear regression models though, and is not generalizeable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094f977-9215-4714-9cee-465fcf84cbb8",
   "metadata": {},
   "source": [
    "## Multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2f97b-52a4-4756-a26f-bc7297ab4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = sklearn.linear_model.LinearRegression()\n",
    "X = dfsmall[['Honey','Brood care','Frame 5','Age']]\n",
    "y = dfsmall[['Dispersion (avg)','Median speed']]\n",
    "# use this to normalize the input/output values before fitting\n",
    "X = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "y = (y-np.mean(y,axis=0))/np.std(y,axis=0)\n",
    "reg.fit(X,y)\n",
    "print ('Coefficients: ', reg.coef_)\n",
    "print ('Intercept: ',reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4a6a5-2e57-46a1-b60b-e7ee0ec7fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X)\n",
    "print('r^2 score:',sklearn.metrics.r2_score(y,y_pred))\n",
    "print('MSE:',sklearn.metrics.mean_squared_error(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804269ee-4262-4789-9063-1623d7cbd03c",
   "metadata": {},
   "source": [
    "### *Q - Regression\n",
    "Consider a regression model where we try to predict age, based on space use\n",
    "Fit a regression with 'Frame 5' as the X variable, and 'Age' as the y variable.\\\n",
    "Then fit a multiple regression with ['Frame 5','Brood care','Honey'] as the X variables, keeping 'Age' as the y variable.\\\n",
    "Compare the performance of these fits in terms of R^2 and MSE.  Comparing coefficients, is the coefficient associated with 'Frame 5' the same, when the multiple regression is fit?  Would you say this is a \"good\" predictor of age?  Which variable is the strongest predictor of age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aec313-bbd1-402a-99ee-5f55e6762dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b489ba-60ad-4c38-af97-07dc429c0755",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbaa17-c244-4396-9d79-5275b9a33bd8",
   "metadata": {},
   "source": [
    "**Representation**\n",
    "* Different models we'll look at here: Logistic regression, random forest, decision tree, state vector machine\n",
    "\n",
    "**Optimization**\n",
    "* Other than special cases, numerical optimization/minimization routines are used.  For example, gradient descent, Newton's method, and others.\n",
    "\n",
    "**Evaluation**\n",
    "* Confusion matrix:  True positive, false positive, true negative, false negative (and generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbc624-6c19-4b6e-a92d-f2ffc200e7da",
   "metadata": {},
   "source": [
    "Here's an introductory description of these different classifier models:\n",
    "\n",
    "<font size=\"5\">1. Logistic Regression: </font>\n",
    "\n",
    "**Overview:**\n",
    "Logistic Regression is a linear model used for binary and multiclass classification problems. Despite its name, it's a classification algorithm, not a regression one. It models the probability of an instance belonging to a particular class using the logistic function.\n",
    "\n",
    "**Key Points:**\n",
    "- Suitable for binary and multiclass classification.\n",
    "- Output is transformed using the logistic function to provide probabilities.\n",
    "- Parameters are learned using optimization techniques like gradient descent.\n",
    "- Interpretable coefficients indicate the impact of features on the prediction.\n",
    "\n",
    "<font size=\"5\">2. Decision Tree: </font>\n",
    "\n",
    "**Overview:**\n",
    "A Decision Tree is a non-linear model used for both classification and regression tasks. It makes decisions based on a series of questions and splits the data into subsets until a decision is reached.\n",
    "\n",
    "**Key Points:**\n",
    "- Hierarchical structure of nodes, branches, and leaves.\n",
    "- Decision-making process based on feature thresholds.\n",
    "- Prone to overfitting, but techniques like pruning can be applied.\n",
    "- Visual representation makes it easy to interpret.\n",
    "\n",
    "<font size=\"5\">3. Random Forest:</font>\n",
    "\n",
    "**Overview:**\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "**Key Points:**\n",
    "- Ensemble of decision trees improves accuracy and reduces overfitting.\n",
    "- Each tree is trained on a random subset of features and data points.\n",
    "- Robust to outliers and noise.\n",
    "- Provides feature importance scores.\n",
    "\n",
    "<font size=\"5\">4. Support Vector Machine (SVM): </font>\n",
    "\n",
    "**Overview:**\n",
    "Support Vector Machine is a powerful and versatile supervised learning algorithm used for classification and regression tasks. It works by finding a hyperplane that best separates the classes in the feature space.\n",
    "\n",
    "**Key Points:**\n",
    "- Effective in high-dimensional spaces.\n",
    "- Kernel trick allows handling non-linear decision boundaries.\n",
    "- Margin maximization ensures robust generalization.\n",
    "- Can be used for both binary and multiclass classification.\n",
    "\n",
    "<font size=\"5\">Brief Comparison:</font>\n",
    "\n",
    "- **Interpretability:**\n",
    "  - Logistic Regression: Coefficients indicate feature impact.\n",
    "  - Decision Tree: Easily interpretable with visual representation.\n",
    "  - Random Forest: Provides feature importance scores.\n",
    "  - SVM: Decision boundaries may be less interpretable.\n",
    "\n",
    "- **Handling Non-linearity:**\n",
    "  - Logistic Regression: Assumes linear relationship between features.\n",
    "  - Decision Tree: Captures non-linear relationships naturally.\n",
    "  - Random Forest: Aggregates non-linearities from multiple trees.\n",
    "  - SVM: Uses kernel trick to handle non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ce019-a850-4ad7-b660-2fde5eea88af",
   "metadata": {},
   "source": [
    "<font size=\"5\">Evaluation metrics</font>\n",
    "\n",
    "*from CognitiveClass.ai*\n",
    "Note that all of these are based on the confusion matrix\n",
    "\n",
    "- __Precision__ is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP / (TP + FP)\n",
    "\n",
    "- __Recall__ is true positive rate. It is defined as: Recall =  TP / (TP + FN)\n",
    "  \n",
    "- __F1 score:__\n",
    "The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "- __Accuracy:__  the average fraction predicted correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed67fd-532f-45df-9d36-fbcba7bb4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a3a50-67da-4570-83d3-779339ec9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohorts 8 and 10 are honey bee drones (male reproductives)\n",
    "# create a column with true/false for whether the bee is a drone\n",
    "df_classify = df[df['Cohort ID'].isin([7,8,9,10])].copy()\n",
    "df_classify['IsDrone'] = df_classify['Cohort ID'].isin([8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8115391-d9a2-4ca3-8f60-17765b2b0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input matrix X and the output prediction y.  The input matrix uses a selection of columns from the data\n",
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_classify[input_metrics]\n",
    "# give y text labels, because then it will be easier to remember what is what\n",
    "y = np.tile('Worker',len(X))\n",
    "y[df_classify['IsDrone']] = 'Drone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad54da0-ac69-4668-b585-c153cffce0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit different classifier models:  Logistic regression, random forest, decision tree, state vector machine\n",
    "clf = LogisticRegression(class_weight='balanced').fit(X,y)\n",
    "clf_rf = RandomForestClassifier(n_estimators=2,max_depth=3,class_weight='balanced').fit(X,y)\n",
    "clf_dt = DecisionTreeClassifier(max_depth=3,class_weight='balanced').fit(X,y)\n",
    "clf_svm = svm.SVC(kernel='rbf').fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24e055-debc-45da-b338-77c775615d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example the predictions of the logistic regression classifier\n",
    "predictions = clf.predict(X)\n",
    "\n",
    "f,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "a=ax[0]\n",
    "cm = sklearn.metrics.confusion_matrix(y, predictions)\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_,)\n",
    "disp.plot(ax=a)\n",
    "a.set_title('Non-normalized (counts)',fontsize=14)\n",
    "\n",
    "a=ax[1]\n",
    "cm = sklearn.metrics.confusion_matrix(y, predictions,normalize='true')\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_,)\n",
    "disp.plot(ax=a)\n",
    "a.set_title('Normalized (fraction)',fontsize=14)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e970a7-23fd-41a4-99a7-3976bb2554f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a function that does this, so we can call it easier to compare models\n",
    "def plotconfusionmatrix(clf,X,y):\n",
    "    predictions = clf.predict(X)\n",
    "    \n",
    "    f,ax = plt.subplots(1,2,figsize=(10,4))\n",
    "    \n",
    "    a=ax[0]\n",
    "    cm = sklearn.metrics.confusion_matrix(y, predictions)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_,)\n",
    "    disp.plot(ax=a)\n",
    "    a.set_title('Non-normalized (counts)',fontsize=14)\n",
    "    \n",
    "    a=ax[1]\n",
    "    cm = sklearn.metrics.confusion_matrix(y, predictions,normalize='true')\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_,)\n",
    "    disp.plot(ax=a)\n",
    "    a.set_title('Normalized (fraction)',fontsize=14)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da317e7-9332-46c9-8824-73f62cec061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the different model fits, and create comparison plots\n",
    "for c,modelname in zip([clf,clf_rf,clf_dt,clf_svm],['Logistic regression','Random forest','Decision tree','SVM']):\n",
    "    plotconfusionmatrix(c,X,y)\n",
    "    plt.suptitle(modelname,fontsize=22)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1797d62-1ba5-43af-b741-361cbe705913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "\n",
    "# first, use the LabelEncoder(), which simply assigns integer values to the different classes (they are currently stored as string)\n",
    "# Then, fit the label encoder on the true labels and transform both true and predicted labels\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "# Evaluate for the logistic regression model and print results\n",
    "predictions = clf.predict(X)\n",
    "predictions_enc = label_encoder.transform(predictions)\n",
    "\n",
    "# Accuracy is another score:  the average fraction predicted correctly\n",
    "print('Accuracy: ',sklearn.metrics.accuracy_score(y_enc,predictions_enc))\n",
    "# precision and recall\n",
    "print('Precision: ',sklearn.metrics.precision_score(y_enc,predictions_enc))\n",
    "print('Recall: ',sklearn.metrics.recall_score(y_enc,predictions_enc))\n",
    "\n",
    "# now, other metrics can be calulated\n",
    "print('F1 score:',sklearn.metrics.f1_score(y_enc,predictions_enc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa68c5e-a36c-43c0-bf4a-eb615ce9b824",
   "metadata": {},
   "source": [
    "Loop through the different models, and store the results for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e514ff-bc54-47e5-883f-b3b2b0bf15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "models = [clf,clf_rf,clf_dt,clf_svm]\n",
    "modelnames = ['Logistic regression','Random forest','Decision tree','SVM']\n",
    "df_scores = pd.DataFrame(columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "\n",
    "for c,modelname in zip(models,modelnames):\n",
    "    predictions = c.predict(X)\n",
    "    predictions_enc = label_encoder.transform(predictions)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_enc,predictions_enc)\n",
    "    precision = sklearn.metrics.precision_score(y_enc,predictions_enc)\n",
    "    recall  = sklearn.metrics.recall_score(y_enc,predictions_enc)\n",
    "    f1 = sklearn.metrics.f1_score(y_enc,predictions_enc)\n",
    "    df_scores.loc[len(df_scores)] = [modelname,accuracy,precision,recall,f1]\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eede3f6-8c19-4a03-9f19-af7f60afdc37",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60560713-1c83-4046-a559-ce5dfdf086a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Feature importance can be assessed in each of these models.  Here is an overview\n",
    "\n",
    "<font size=4>Logistic Regression</font>\n",
    "- **Coefficients as Importance**: In Logistic Regression, the coefficients of the model (obtained via `model.coef_` in scikit-learn) represent the importance of each feature.\n",
    "- **Interpretation**: A larger absolute value of a coefficient indicates a stronger influence of the corresponding feature on the model's predictions. The sign (positive/negative) of the coefficient indicates the direction of the influence.\n",
    "\n",
    "<font size=4>Decision Tree</font>\n",
    "- **Feature Importance Attribute**: Decision Trees in scikit-learn provide a `feature_importances_` attribute, which gives a score for each feature’s importance in making predictions.\n",
    "- **Scaled Scores**: These importance scores are scaled such that they sum to 1. Each score represents the contribution of the feature to the decision-making process of the tree.\n",
    "\n",
    "<font size=4>Random Forest</font>\n",
    "- **Aggregated Feature Importance**: Similar to Decision Trees, Random Forests also use the `feature_importances_` attribute. This is an average of the feature importance scores across all trees in the forest.\n",
    "- **Robustness**: Feature importance from a Random Forest is often considered more robust than from a single Decision Tree as it aggregates information over many trees.\n",
    "\n",
    "<font size=4>SVM (Support Vector Machine)</font>\n",
    "- **Linear SVM**: For linear SVM models (using a linear kernel), feature importance can be assessed by looking at the coefficients of the support vectors, similar to Logistic Regression.\n",
    "- **Non-linear SVM**: Interpreting feature importances in non-linear SVMs is complex and often not feasible due to the kernel trick. The model's decision function depends on the dot product of input features and support vectors, which does not provide straightforward feature importance.\n",
    "\n",
    "<font size=4>General Notes</font>\n",
    "- **Contextual Interpretation**: The interpretation of feature importance can depend on the context and the specific preprocessing steps taken (like feature scaling).\n",
    "- **Model-Specific**: Feature importance is model-specific and can vary significantly between different types of models.\n",
    "- **Absolute vs. Relative Importance**: For Logistic Regression and Linear SVM, the coefficients represent the absolute importance assuming all features are on the same scale, whereas for Decision Trees and Random Forests, the importance scores are relative to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3f0df-0900-4008-b989-b365e9118976",
   "metadata": {},
   "source": [
    "Evaluate and print feature importance for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d1522-f002-4b07-8b08-5004cabfa084",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'Logistic regression'\n",
    "# Convert to a DataFrame for easier visualization\n",
    "feature_importances = pd.DataFrame({'feature': clf.feature_names_in_, 'coefficient': clf.coef_[0]})\n",
    "# Sort by importance\n",
    "feature_importances = feature_importances.sort_values(by='coefficient', ascending=False)\n",
    "print('\\n',modelname)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a6597-776a-4bd4-8181-31b1ba70fcb0",
   "metadata": {},
   "source": [
    "Evaluate and print feature importance for Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2dcee-35f7-4bf2-a74d-e8fbd73926d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,modelname in zip([clf_dt,clf_rf],['Decision tree','Random forest']):\n",
    "    # Convert to a DataFrame for easier visualization\n",
    "    feature_importances = pd.DataFrame({'feature': c.feature_names_in_, 'importance': c.feature_importances_})\n",
    "    # Sort by importance\n",
    "    feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "    print('\\n',modelname)\n",
    "    print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b737647-b6f3-45f0-9b1d-c53cc6494339",
   "metadata": {},
   "source": [
    "For SVM, we can only evaluate if we use a linear kernel.  Refit, and then evaluate.  (code takes awhile to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441d7c1-78c4-47de-abee-89fdb964d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'SVM - linear'\n",
    "clf_svm = svm.SVC(kernel='linear').fit(X, y) \n",
    "feature_importances = pd.DataFrame({'feature': clf_svm.feature_names_in_, 'coefficient': clf_svm.coef_[0]})\n",
    "# Sort by importance\n",
    "feature_importances = feature_importances.sort_values(by='coefficient', ascending=False)\n",
    "print('\\n',modelname)\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a46bfc-9a7a-488e-aaf3-7a9bf6ca4345",
   "metadata": {},
   "source": [
    "## Multiple classes (more than two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a6553-2dbe-4acf-896c-846cc7b8ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohorts 8 and 10 are honey bee drones (male reproductives)\n",
    "df_classify = df[df['Cohort ID'].isin([7,8,9,10])].copy()\n",
    "df_classify['IsDrone'] = df_classify['Cohort ID'].isin([8,10])  # define this for visualization, but here we will use cohort id as the prediction classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669251a-5a8f-4505-ba07-73ca56bb3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input matrix X and the output (classes) list y\n",
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_classify[input_metrics]\n",
    "y = df_classify['Cohort ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0922250-abc7-4d68-86b4-f2152bd35a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit different classifier models\n",
    "\n",
    "# In the optionts, if class_weight is set to \"balanced,\" the algorithm sets the weights inversely proportional to the class frequencies.  \n",
    "# If the number of counts of different classes is very different, it is a good idea to use this option and compare.\n",
    "clf = LogisticRegression(class_weight='balanced').fit(X,y)\n",
    "clf_rf = RandomForestClassifier(n_estimators=2,max_depth=3,class_weight='balanced').fit(X,y)\n",
    "clf_dt = DecisionTreeClassifier(max_depth=3,class_weight='balanced').fit(X,y)\n",
    "clf_svm = svm.SVC(kernel='rbf').fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817feb2d-c859-4b3a-8841-8a047d4348eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the label encoder to get ordered integer values for the classes\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "# make a list to loop through and compare the confusion matrix for different model fits\n",
    "models = [clf,clf_rf,clf_dt,clf_svm]\n",
    "modelnames = ['Logistic regression','Random forest','Decision tree','SVM']\n",
    "df_scores = pd.DataFrame(columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "\n",
    "for c,modelname in zip(models,modelnames):\n",
    "    plotconfusionmatrix(c,X,y)  # note that this function was defined above in the previous section\n",
    "    plt.suptitle(modelname,fontsize=22)\n",
    "    plt.show()\n",
    "\n",
    "    predictions = c.predict(X)\n",
    "    predictions_enc = label_encoder.transform(predictions)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_enc,predictions_enc)\n",
    "    precision = sklearn.metrics.precision_score(y_enc,predictions_enc,average='weighted')\n",
    "    recall  = sklearn.metrics.recall_score(y_enc,predictions_enc,average='weighted')\n",
    "    f1 = sklearn.metrics.f1_score(y_enc,predictions_enc,average='weighted')\n",
    "    df_scores.loc[len(df_scores)] = [modelname,accuracy,precision,recall,f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17ae07-f437-4a58-b924-007653aa198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores for the different model fits\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805e67d-f426-43b6-8f44-92bf031a2378",
   "metadata": {},
   "source": [
    "### *Q - Classification\n",
    "The above example uses the following code to fit and evaluate the accuracy of the Logistic regression model.  This uses multiple inputs.\\\n",
    "- Instead of multiple inputs, how is the accuracy affected if each single metric is fit separately?\n",
    "- How do the different accuracies of fit results compare with the feature importance inferred above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548eb3b-27b6-48dd-9382-ed7ab1a055c8",
   "metadata": {},
   "source": [
    "Code from the example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0e3a5-4926-4918-bb9c-2610871ee731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohorts 8 and 10 are honey bee drones (male reproductives)\n",
    "# create a column with true/false for whether the bee is a drone\n",
    "df_classify = df[df['Cohort ID'].isin([7,8,9,10])].copy()\n",
    "df_classify['IsDrone'] = df_classify['Cohort ID'].isin([8,10])\n",
    "\n",
    "# define the input matrix X and the output prediction y.  The input matrix uses a selection of columns from the data\n",
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_classify[input_metrics]\n",
    "# give y text labels, because then it will be easier to remember what is what\n",
    "y = np.tile('Worker',len(X))\n",
    "y[df_classify['IsDrone']] = 'Drone'\n",
    "\n",
    "# fit the classifier model\n",
    "clf = LogisticRegression(class_weight='balanced').fit(X,y)\n",
    "\n",
    "# Evaluate accuracy\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "# Evaluate for the logistic regression model and print results\n",
    "predictions = clf.predict(X)\n",
    "predictions_enc = label_encoder.transform(predictions)\n",
    "\n",
    "# Accuracy is another score:  the average fraction predicted correctly\n",
    "print('Accuracy: ',sklearn.metrics.accuracy_score(y_enc,predictions_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a019cf-01eb-4e8d-bd07-3fa15754a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e72007-d472-4786-85f2-d45f2e1dd5c6",
   "metadata": {},
   "source": [
    "## Train/test division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd1cfe-1a7e-4bc9-853f-13b00d6c8109",
   "metadata": {},
   "source": [
    "We just fit each model above - we didn't think about the parameters or the possibility of overfitting!  for the decision tree and random forest classifiers, the max_depth is a key parameter, and we can check overfitting by using a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca615d-e120-4368-9930-f10426b83e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_classify[input_metrics]\n",
    "\n",
    "## use this for drone/worker classification\n",
    "y = np.tile('Worker',len(X))\n",
    "y[df_classify['IsDrone']] = 'Drone'\n",
    "\n",
    "## use this for cohort ID classification\n",
    "y = df_classify['Cohort ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372fc96-d31d-420b-b2e5-fb32d6ba7464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a train/test split using built-in scikit-learn function\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=5)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e359-cad2-491f-ac4d-ca4c758923c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the scores on a classifier model more easily\n",
    "def get_scores(c, X, y):\n",
    "    \"\"\"\n",
    "    Calculate various classification scores for a fitted classifier model.\n",
    "\n",
    "    Parameters:\n",
    "    - c (classifier): A fitted classifier model.\n",
    "    - X (array-like): Input data matrix.\n",
    "    - y (array-like): Classes list.\n",
    "\n",
    "    Returns:\n",
    "    - scores (list): List containing accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode the classes using LabelEncoder\n",
    "    label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "    y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Make predictions using the fitted model\n",
    "    predictions = c.predict(X)\n",
    "    predictions_enc = label_encoder.transform(predictions)\n",
    "\n",
    "    # Choose 'binary' or 'weighted' averaging based on the number of unique classes\n",
    "    average = 'binary' if len(np.unique(y_enc)) == 2 else 'weighted'\n",
    "\n",
    "    # Calculate classification scores\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_enc, predictions_enc)\n",
    "    precision = sklearn.metrics.precision_score(y_enc, predictions_enc, average='weighted', zero_division=np.nan)\n",
    "    recall = sklearn.metrics.recall_score(y_enc, predictions_enc, average='weighted')\n",
    "    f1 = sklearn.metrics.f1_score(y_enc, predictions_enc, average=average)\n",
    "\n",
    "    # Return the scores as a list\n",
    "    return [accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98edde2-7e68-43d5-9eb6-c998031a4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new dataframe to save the scores\n",
    "df_scores = pd.DataFrame(columns=['Model','Depth','Accuracy','Precision','Recall','F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2dc428-3e09-41aa-ad4b-38079f4fdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "clf = LogisticRegression(class_weight='balanced').fit(x_train,y_train)\n",
    "df_scores.loc[len(df_scores)] = ['Logistic regression',None] + get_scores(clf,x_test,y_test)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f699ac8-1f63-42ff-86c6-363acc5ae11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the depth of the tree and calculate the scores\n",
    "# note that the fitting is done on the training set, and the evalulation on the test set\n",
    "for depth in range(1,30,1):\n",
    "    # fit models\n",
    "    clf_rf = RandomForestClassifier(\n",
    "        n_estimators=20,max_depth=depth,class_weight='balanced').fit(x_train,y_train)\n",
    "    clf_dt = DecisionTreeClassifier(\n",
    "        max_depth=depth,class_weight='balanced').fit(x_train,y_train)\n",
    "    # calculate accuracy and save\n",
    "    df_scores.loc[len(df_scores)] = ['RF',depth] + get_scores(clf_rf,x_test,y_test)\n",
    "    df_scores.loc[len(df_scores)] = ['DT',depth] + get_scores(clf_dt,x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a1c73-9722-4f13-a807-ad86a0430d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17c9bb-f46b-4cba-8e3e-6ddf8fb2c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy of the test set as a function of the depth of the tree\n",
    "for modelname in ['DT','RF']:\n",
    "    dfsel = df_scores[df_scores['Model']==modelname]\n",
    "    plt.plot(dfsel['Depth'],dfsel['Accuracy'],label=modelname)\n",
    "plt.axhline(df_scores.loc[df_scores['Model']=='Logistic regression']['Accuracy'].values,linestyle='--',c='gray',label='Logistic regression')\n",
    "plt.xlabel('Depth',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11fb24-14d0-4609-8d8f-c94f47b0a1ec",
   "metadata": {},
   "source": [
    "For this example, it looks like it is not overfitting, because the accuracy plateaus and doesn't decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590906-9b42-4e3e-be74-9b92555802ce",
   "metadata": {},
   "source": [
    "# Regression and Classification using deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c65ac-5dea-4100-826b-359f535fc4ef",
   "metadata": {},
   "source": [
    "What is deep learning, and how is it different from the above examples?\\\n",
    "--> A deep learning classifier consists of multiple layers of interconnected nodes, enabling it to automatically learn hierarchical representations of features. Deep learning models can capture complex patterns, and can perform well in tasks with high-dimensional data and intricate relationships.  However, the results and the importance of different features can be more difficult to interpret.\\\n",
    "\\\n",
    "The 3 core aspects of representation, fitting, and evaluation still apply. For deep learning models, the evaluation methods are the same.  But the model is much more complicated with many more parameters - so the representation and fitting steps are much more involved.\\\n",
    "\\\n",
    "Using sklearn, we can have a high-level interface for classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c2eb2-5e68-4ad0-9bde-2d0b6718105a",
   "metadata": {},
   "source": [
    "## Classification with neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397c62a-0e29-4949-aa83-68c5c31e5e66",
   "metadata": {},
   "source": [
    "The Multi-Layer Perceptron (MLP) classifier is a type of artificial neural network designed for supervised learning tasks, particularly classification. \\\n",
    "\\\n",
    "MLP consists of an input layer, one or more hidden layers, and an output layer.\n",
    "Nodes in each layer are interconnected, forming a densely connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8191226-6dd4-4d2a-9f97-c38cc7251fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a399f6d-5b5e-4647-8cb7-48f595b1672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_classify[input_metrics]\n",
    "\n",
    "## use this for drone/worker classification\n",
    "y = np.tile('Worker',len(X))\n",
    "y[df_classify['IsDrone']] = 'Drone'\n",
    "\n",
    "## use this for cohort ID classification\n",
    "y = df_classify['Cohort ID']\n",
    "\n",
    "# train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=5)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9a997-ea8f-493b-b8ce-c6915d147ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multi-Layer Perceptron (MLP) classifier with a single hidden layer of 100 notes\n",
    "# - 'hidden_layer_sizes': Number of units in each hidden layer, here a single layer with 100 nodes.\n",
    "# - 'solver': Optimization algorithm for weight updates, 'adam' is a popular choice.\n",
    "# - 'activation': Activation function for hidden layer units, 'relu' (Rectified Linear Unit) is commonly used.\n",
    "# - 'random_state': Seed for reproducibility of results.\n",
    "clf_nn = MLPClassifier(hidden_layer_sizes=[100], solver='adam', activation='relu', random_state=0)\n",
    "\n",
    "# Fit the MLP classifier to the training data (x_train and y_train)\n",
    "clf_nn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66ce7f-6b5b-4602-a8d6-8084978930b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the different scores for this classifier fit\n",
    "get_scores(clf_nn,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7cfe1-0c97-4e15-80f6-5fe4ea8f393b",
   "metadata": {},
   "source": [
    "What is the effect on different numbers of layers, different layer sizes, and difference activation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90040d-3a74-4282-a879-3453587f3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new dataframe to save the scores\n",
    "df_scores = pd.DataFrame(columns=['Model','layer_size','num_layers','Accuracy','Precision','Recall','F1'])\n",
    "\n",
    "# loop through different choices\n",
    "for act_fn in ['logistic', 'tanh', 'relu']:\n",
    "    for layer_size in [1,10,20]:\n",
    "        for num_layers in [1,2,3]:\n",
    "            print('fitting: ',(act_fn,layer_size,num_layers))\n",
    "            clf_nn = MLPClassifier(hidden_layer_sizes = np.tile(layer_size,num_layers), solver='adam',\n",
    "                                   activation=act_fn,\n",
    "                         random_state = 0).fit(x_train, y_train)\n",
    "            scores = get_scores(clf_nn,x_test,y_test)\n",
    "            df_scores.loc[len(df_scores)] = ['clf-'+act_fn,layer_size,num_layers]+scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679047fb-ae73-40d3-b6c7-6ddb768a4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda402df-4bc7-475c-826c-e36803759cad",
   "metadata": {},
   "source": [
    "This classifier is not performing very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce629c-0cf5-49c1-b994-04c666ca9f61",
   "metadata": {},
   "source": [
    "## Regression with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07649ef-3996-461b-b95a-09a1fbcc69a7",
   "metadata": {},
   "source": [
    "The MLPRegressor (Multi-Layer Perceptron Regressor) is a type of artificial neural network used for regression tasks. It shares similarities with the MLPClassifier but is designed to predict continuous numerical values rather than class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01861f81-bbd8-4d8d-a218-170e382dd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b42660-8df4-4f27-a877-2574cf9d349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pick some values from the dataframe to create the input matrix X and the output matrix y\n",
    "X = dfsmall[['Honey','Brood care','Frame 5','Age']]\n",
    "y = dfsmall[['Dispersion (avg)','Median speed']]\n",
    "# filter for nans\n",
    "sel = np.logical_not(np.any(X.isna(),axis=1) | np.any(y.isna(),axis=1) )\n",
    "X = X[sel]\n",
    "y = y[sel]\n",
    "\n",
    "# use this to normalize the input/output values before fitting\n",
    "X = (X-np.mean(X,axis=0))/np.std(X,axis=0)\n",
    "y = (y-np.mean(y,axis=0))/np.std(y,axis=0)\n",
    "\n",
    "# train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3938a3a9-aeca-4bc2-b5ef-d313d333c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLPRegressor model with two hidden layers, each containing 20 neurons\n",
    "# - 'hidden_layer_sizes': List specifying the number of neurons in each hidden layer, here two layers with 20 neurons each.\n",
    "# - 'activation': Activation function for hidden layer neurons, 'tanh' (Hyperbolic Tangent) is chosen for non-linearity.\n",
    "# - 'max_iter': Maximum number of iterations during optimization, set to 500 for convergence.\n",
    "reg = MLPRegressor(hidden_layer_sizes=[20, 20], activation='tanh', max_iter=500)\n",
    "\n",
    "# Fit the MLPRegressor model to the training data (x_train and y_train)\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cda5e-03a4-40f5-99d5-6343bd625d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate prediction scores\n",
    "y_pred = reg.predict(x_test)\n",
    "print('r^2 score:',sklearn.metrics.r2_score(y_test,y_pred))\n",
    "print('MSE:',sklearn.metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5cbb26-187c-4a15-8f69-6d54af45504c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets again loop through and see the effects of parameters\n",
    "\n",
    "# initialize a new dataframe to save the scores\n",
    "df_scores = pd.DataFrame(columns=['Model','layer_size','num_layers','r2','MSE'])\n",
    "# loop through difference choices\n",
    "for act_fn in ['logistic', 'tanh', 'relu']:\n",
    "    for layer_size in [1,10,20]:\n",
    "        for num_layers in [1,2,3]:\n",
    "            print('fitting: ',(act_fn,layer_size,num_layers))\n",
    "            reg = MLPRegressor(hidden_layer_sizes=[20,20],\n",
    "                               activation='tanh',max_iter=500).fit(x_train,y_train)\n",
    "            y_pred = reg.predict(x_test)\n",
    "            r2 = sklearn.metrics.r2_score(y_test,y_pred)\n",
    "            mse = sklearn.metrics.mean_squared_error(y_test,y_pred)\n",
    "            df_scores.loc[len(df_scores)] = ['reg-'+act_fn,layer_size,num_layers,r2,mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a416a-3c73-4288-a72b-65606e1b2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde1ef4-656c-4110-96d0-495fbe59949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a regular linear multiple regression model to compare with\n",
    "reg = sklearn.linear_model.LinearRegression().fit(x_train,y_train)\n",
    "y_pred = reg.predict(x_test)\n",
    "print('r^2 score:',sklearn.metrics.r2_score(y_test,y_pred))\n",
    "print('MSE:',sklearn.metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853049d-87fb-4a0a-b931-1a6e4a6a7dc2",
   "metadata": {},
   "source": [
    "The nonlinear model does a lot better in predicting quantities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb99502-c4b2-469a-a928-ae736d0c078e",
   "metadata": {},
   "source": [
    "### *Q Deep learning models - reflection.\n",
    "Considering the different classification models (deep learning vs not), which would you choose for an application of data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288f6a1-6a75-4b7f-8074-bf1a250b802b",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4658bba-fcd6-4ed2-9ea1-68cb202dc988",
   "metadata": {},
   "source": [
    "Unsupervised learning seeks patterns in the data that have not previously been specified.  Two main types:\n",
    "- Dimensionality reduction\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0307e29-7a53-4fe6-80e9-a82003b24ba4",
   "metadata": {},
   "source": [
    "## Dimensionality reduction:  Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45054b1f-3f48-413c-a57a-10d1e641c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c190411-f89e-4618-8c39-febb3d9de5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohorts 8 and 10 are honey bee drones (male reproductives)\n",
    "# create a subset dataframe just containing drones and workers (dw)\n",
    "df_dw = df[df['Cohort ID'].isin([7,8,9,10])].copy()\n",
    "df_dw['IsDrone'] = df_dw['Cohort ID'].isin([8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a3cbb-42a7-4b24-bdc9-b4f3df9ee026",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metrics = ['Num. observations', 'Honey', 'Brood care', 'Pollen','Frame 5','Median speed','Dispersion (avg)', 'Exit distance (median)']\n",
    "X = df_dw[input_metrics]\n",
    "# take a subset for speeding up calculations in the examples\n",
    "X = X[::3]\n",
    "# standardize the columns of X\n",
    "X = (X-np.mean(X,axis=0))/np.std(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a0705-8d7c-4f39-a981-7855b4ba68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA object and fit it to the input data X\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# Retrieve the principal components (loadings) from the fitted PCA model\n",
    "vh = pca.components_\n",
    "\n",
    "# Transform the original data X into the new coordinate system defined by the principal components\n",
    "u = pca.transform(X)\n",
    "\n",
    "# Retrieve the explained variance ratio of each principal component\n",
    "pcavar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858c5fe-8f6e-43a1-8d95-adb7f7978a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame 'df_pca' from the transformed data 'u'\n",
    "# - 'u': Transformed data from the PCA model, representing the principal components.\n",
    "# - 'columns': Named as 'PCA 1', 'PCA 2', ..., corresponding to each principal component.\n",
    "# - 'index': Preserves the original index from the input data 'X'.\n",
    "df_pca = pd.DataFrame(u,columns=['PCA '+str(i+1) for i in range(u.shape[1])],index=X.index)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e64634-6e5d-4987-8def-d690d002e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize what the PCA components are using seaborn barplot, and show the respective fraction of variance explained of each\n",
    "num_to_show = 5\n",
    "f,ax = plt.subplots(1,num_to_show,sharex=True,sharey=True)\n",
    "f.set_size_inches(2*num_to_show,3)\n",
    "for i,a in enumerate(ax):\n",
    "    sns.barplot(data=vh[i], orient=\"h\",ax=a)\n",
    "    a.set_title('PCA '+str(i+1)+': '+str(np.round(pcavar[i]*100,1))+'%',fontsize=16)\n",
    "ax[0].set_yticks(range(len(X.columns)))\n",
    "ax[0].set_yticklabels(X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38aeb3-d670-4805-b652-c22972109d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the projection onto the first two PCA axes.\n",
    "sns.scatterplot(x='PCA 1',y='PCA 2',data=df_pca.join(df_dw),hue='IsDrone',alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a916ccb-897b-4ff8-ac1a-fd17d714c792",
   "metadata": {},
   "source": [
    "### *Q: PCA\n",
    "If 'Frame 5' is removed from the inputs, how do the PCA vectors change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d2669-2f8a-4c4a-81ca-31e743a816b4",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with tSNE (t stochastic neighbor embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943fd8b-d336-4833-a7fc-b44f24983085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7959d8d-10f0-4212-868d-b64666e4a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE dimensionality reduction on the input data 'X'\n",
    "# - 'n_components': Number of dimensions in the embedded space, set to 2 for 2D visualization.\n",
    "# - 'init': Initialization method for the optimization, using PCA for better convergence.\n",
    "# - 'perplexity': Balance between preserving local and global structures, typically set between 5 and 50.\n",
    "X_embedded = TSNE(n_components=2, init='pca', perplexity=30).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c939f05-9d87-4d2f-a299-56773ecd41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "\n",
    "# Create a DataFrame 'df_toplot' combining the t-SNE embedded data and additional features\n",
    "# - 'X_embedded': 2D embedded data obtained from t-SNE.\n",
    "# - 'columns': Labeled as 'TSNE 1' and 'TSNE 2' for the two dimensions of the embedded space.\n",
    "# - 'index': Preserves the original index from the input data 'X'.\n",
    "# - 'join(df_dw)': Joins additional features from DataFrame 'df_dw' to the t-SNE embedded data.\n",
    "df_toplot = pd.DataFrame(X_embedded, columns=['TSNE 1', 'TSNE 2'], index=X.index).join(df_dw)\n",
    "\n",
    "# Create a scatter plot to visualize the t-SNE representation\n",
    "# - 'x=' and 'y=': Specify the columns for the x and y-axis, representing the t-SNE dimensions.\n",
    "# - 'data=df_toplot': Data source for the scatter plot.\n",
    "# - 'hue='IsDrone'': Color points based on the 'IsDrone' feature.\n",
    "sns.scatterplot(x='TSNE 1',y='TSNE 2',data=df_toplot,hue='IsDrone',alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd5f19-13bc-4364-80af-fa68a6232e00",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ff4b2-6091-48f1-b46d-590a89eb5929",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc407a-26cc-412b-b101-9b40505ad004",
   "metadata": {},
   "source": [
    "Here are three different clustering methods we'll compare:\\\n",
    "\\\n",
    "<font size=\"5\">1. KMeans</font>\n",
    "\n",
    "**Overview:**\n",
    "- **Type:** Partitioning-based clustering.\n",
    "- **Method:** Divides data into 'k' clusters based on similarity.\n",
    "- **Objective:** Minimizes the sum of squared distances between data points and their cluster centroids.\n",
    "- **Parameters:** Number of clusters 'k' needs to be specified.\n",
    "- **Strengths:**\n",
    "  - Simple and computationally efficient.\n",
    "  - Works well for well-separated, spherical clusters.\n",
    "\n",
    "<font size=\"5\">2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):</font>\n",
    "\n",
    "**Overview:**\n",
    "- **Type:** Density-based clustering.\n",
    "- **Method:** Identifies clusters based on areas of high data point density.\n",
    "- **Objective:** Forms clusters by connecting data points within a specified neighborhood density.\n",
    "- **Parameters:** Epsilon (radius for defining neighborhood) and minimum points required to form a dense region.\n",
    "- **Strengths:**\n",
    "  - Can find clusters of arbitrary shapes.\n",
    "  - Robust to noise and outliers.\n",
    "\n",
    "<font size=\"5\">3. AgglomerativeClustering:</font>\n",
    "\n",
    "**Overview:**\n",
    "- **Type:** Hierarchical clustering.\n",
    "- **Method:** Builds a hierarchy of clusters by recursively merging or agglomerating data points.\n",
    "- **Objective:** Creates a dendrogram, allowing the selection of the desired number of clusters.\n",
    "- **Parameters:** Linkage method (e.g., ward, average, complete) and the number of clusters.\n",
    "- **Strengths:**\n",
    "  - Provides a hierarchical structure of clusters.\n",
    "  - Suitable for datasets with nested or hierarchical structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe36ed-0470-416f-a56f-906aa094ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4796c3-2836-40ba-af75-78501dbf30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4  # need to specify this for kmeans and hierarchical clustering\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init='auto')\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=20)\n",
    "dbscan_labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Hierarchical clustering (Agglomerative)\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "agg_labels = agg_clustering.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6194f-900f-4f69-a4b8-1ae4c6a9ec30",
   "metadata": {},
   "source": [
    "use PCA or tSNE embedding to visualize these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedb4bea-dd73-4a05-82a2-59d2aed98cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_toplot, y_toplot = df_pca[['PCA 1','PCA 2']].values.T  # use this to plot on first two PCA axes\n",
    "x_toplot, y_toplot = X_embedded.T  # use this to plot with tSNE embedding\n",
    "\n",
    "f,ax = plt.subplots(1,3,sharex=True,sharey=True)\n",
    "f.set_size_inches(12,4)\n",
    "# loop through the different clustering methods to plot results\n",
    "for a,labels,title in zip(ax,[kmeans_labels,dbscan_labels,agg_labels],['K means','DBScan','Hierarchical clustering']):\n",
    "    sns.scatterplot(x=x_toplot,y=y_toplot,hue=labels,alpha=0.5,ax=a,palette='Set1')\n",
    "    a.set_title(title,fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23d096-2389-406c-b300-588bb2422217",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
